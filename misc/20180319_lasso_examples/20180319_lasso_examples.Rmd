---
title: "LASSO. Example"
output:
  html_document:
    df_print: paged
---

This document shows some examples using LASSO. The content shown is based on ISLR. 

## Example given in ISLR
The example uses the `Hitters` dataset to show parameter estimation using LASSO. We start by loading the following two packages.

```{r}
require(ISLR)
require(glmnet)
```

The dataset is obtained by the following command

```{r}
data(Hitters)
Hitters <- na.omit(Hitters)
dim(Hitters)
```

The structure of the `Hitters` dataframe 

```{r}
str(Hitters)
```

The above output shows that for the example of the `Hitters` data, we still have $n>p$ and still we are using LASSO, to determine which variables are important for a given response variable.

The parameter $\lambda$ will be determined using crossvalidation, hence we have to create a training and a test set.

```{r TrainTestSet}
set.seed (1)
train <- sample (c(TRUE ,FALSE), nrow(Hitters), rep=TRUE)
test  <- (! train )
```

Explanatory variables are assigned to $x$ and repsonse variable is assigned to $y$. 

```{r LassoModelPrep}
x <- model.matrix (Salary ~ ., Hitters)[,-1]
y <- Hitters$Salary
```

Possible values of $\lambda$ are given on a grid between $10^{10}$ and $10^{-2}$. 

```{r GridLambda}
grid <- 10^ seq (10,-2, length =100)
```

The following statements fits the model using LASSO with the training data. 

```{r LassoModel}
lasso.mod <- glmnet (x[train ,],y[train],alpha =1, lambda = grid)
str(lasso.mod)
```

Plots showing the different coefficients and their values as a function of the $L_1$-norm is shown below. 

```{r}
plot(lasso.mod, label = TRUE)
```

Alternatively, we can also plot the coefficients as a function of the value of $\lambda$. 

```{r}
plot(lasso.mod, xvar = "lambda", label = TRUE)
```

To find the best $\lambda$, we run a cross-validation

```{r CrossValidation}
set.seed (1)
cv.out <- cv.glmnet (x[train ,],y[train],alpha =1)
str(cv.out)
```

The best value is obtained by 

```{r}
(bestlam <- cv.out$lambda.min)
```

Doing the fit on all the data using the best $\lambda$ shows us which coefficients will be set to $0$. 

```{r LassoCoeff}
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s=bestlam ) [1:20,]
lasso.coef
```


## Example taken from vignette of `glmnet`
The vignette is opened using the command

```{r, eval=FALSE}
browseVignettes(package = "glmnet")
```

and from the website that is shown, we open the vignette entitled `An Introduction to Glmnet`. Inside that vignette, the dataset `QuickStartExample` is used

```{r}
data(QuickStartExample)
```

The above call to `data` assignes the matrix `x` with dimensions

```{r}
dim(x)
```

as explanatory variables and the matrix `y` with dimensions

```{r}
dim(y)
```

as response variables. 

The following call to `glmnet` fits a GLM to the data

```{r}
fit <- glmnet(x=x, y=y)
```

The result is an object of class `glmnet` and is quite complicated. Hence, we best make use of the provided methods to inspect our results. 


### Plots
One method is the method `plot` to show the values of the coefficients.

```{r}
plot(fit, label = TRUE)
```

Each curve corresponds to a variable. The labels of the variables are shown towards the right-hand side border. The curves show the path of the coefficients against the $L_1$-norm, as $\lambda$ varies. The numbers above show the numbers of non-zero coefficients at the current $\lambda$. 

### Summary
A summary of the results can be produced using the `print`-method

```{r}
print(fit)
```

Although by default `glmnet` will produce $100$ values, the results are ommitted, if the deviance does not change anymore. 

### Coefficients
Coefficients can be obtained for at one or more $\lambda$-values. 

```{r}
coef(fit, s = .1)
```


### Predictions
Predictions can be made for new input data for a range of $\lambda$-values

```{r}
nx <- matrix(rnorm(10*20), 10, 20)
predict(fit, newx = nx, s = c(.1, .05))
```


### Cross validation
When the user just wants one model, cross validation is one possibility to get to that. For that the function `cv.glmnet` is used.

```{r}
cvfit <- cv.glmnet(x = x, y = y)
```

The result can be plotted

```{r}
plot(cvfit)
```

This plot contains the cross-validation curve (red dots) and upper and lower standard deviation curves along the $\lambda$-sequence. Two selected $\lambda$-values are indicated by the vertical dotted lines. The selected $\lambda$-values can be seen by 

```{r}
cvfit$lambda.min
```
where `lambda.min` corresponds to the $\lambda$-value that gives the minimum cross-validation error. The other selected $\lambda$-value is saved under the name of `lambda.1se` 

```{r}
cvfit$lambda.1se
```
which gives the most regularized model such that the error is within one standard error of the error minimum. 

Coefficients can be generated using these selected $\lambda$-values

```{r}
coef(cvfit, s = "lambda.1se")
```
or analogously 

```{r}
coef(cvfit, s = "lambda.min")
```

Prediction can be made on the `cv.glmnet` object for new `x`-values

```{r}
predict(cvfit, newx = x[1:5,], s = "lambda.min")
```

```{r}
predict(cvfit, newx = x[1:5,], s = "lambda.1se")
```


## Application of LASSO to Genomics Data
This section tries to use the LASSO for genomics data. That means we want to test whether the LASSO can handle the situation where $p > n$. Let us start with a moderate example of simulated data.

```{r}
# set the seed for reproducibility
set.seed(8789)
# generate the matrix at a given number of SNP-loci
#  and for a given number of animals
n_nr_snp <- 100
n_nr_animals <- 50
n_min_allele_freq <- 0.1
# generate the matrix using sample()
mat_snp <- matrix(data = sample(c(0,1,2), 
                               n_nr_snp*n_nr_animals, 
                               replace = T, 
                               prob = c((1-n_min_allele_freq)^2,
                                        2*(1-n_min_allele_freq)*n_min_allele_freq,
                                        n_min_allele_freq^2)), 
                 nrow = n_nr_animals, 
                 ncol = n_nr_snp)
# change the coding
mat_snp <- mat_snp - 1
# check dimensions
dim(mat_snp)
```

Für die Simulation der Beobachtungen wählen wir die SNP-Kolonnen mit der meisten Variabilität.

```{r}
n_nr_sign_snp <- 5
vec_snp_sd <- apply(mat_snp, 2, sd)
vec_ord_sd <- order(vec_snp_sd, decreasing = TRUE)
(vec_sign_snp_idx <- vec_ord_sd[1:n_nr_sign_snp])
```

Generating the effects for the SNPs

```{r}
n_snp_a_min <- .1
n_snp_a_max <- 13
(vec_snp_eff <- runif(n = n_nr_sign_snp, min = n_snp_a_min, max = n_snp_a_max))
```

Generating the phenotypes

```{r}
n_true_inter <- -12.4
n_true_sd <- 3.41
vec_y <- crossprod(t(mat_snp[,vec_sign_snp_idx]), vec_snp_eff) + 
  rnorm(n = n_nr_animals, mean = n_true_inter, sd = n_true_sd)
```

### Writing data to a file
In order to have the genotypes and the phenotypes ready for later analysis, we write them to a file. The format of the file is that the first column consists of the phenotypes and all other columns are genotypes. First the phenotypes and the genotypes are bound into one matrix

```{r}
### # bind columns of phenotypes and genotypes together
mat_data <- cbind(vec_y, mat_snp)
dim(mat_data)
```

Combined data is written to the file.

```{r}
### # write data to file
s_lasso_data_filename <- "asmas_w05_u04_lasso.txt"
write.table(mat_data, file = s_lasso_data_filename, quote = FALSE, sep = "\t", row.names = FALSE, col.names = FALSE)
file.info(s_lasso_data_filename)
```


### Fitting the model to the data
Now we try to fit a model using LASSO

```{r}
fit_snp <- glmnet(x = mat_snp, y = vec_y)
```

### Visualisations
The visualisation gives 

```{r}
plot(fit_snp, label = TRUE)
```

Coefficients as a function of $\lambda$

```{r}
plot(fit_snp, xvar = "lambda", label = TRUE)
```

### Results
The results are obtained by

```{r}
print(fit_snp)
```

### Cross-validation
The cross-validation is done to get an estimate of $\lambda$. 

```{r}
cvfitsnp <- cv.glmnet(x = mat_snp, y = vec_y)
```

CV-results are plotted by

```{r}
plot(cvfitsnp)
```

### Coefficients based on special $\lambda$
The two marked $\lambda$-values are 

```{r}
cvfitsnp$lambda.min;cvfitsnp$lambda.1se
```

The coefficients for the two marked $\lambda$-values are obtained by

```{r}
coefmin <- coef(cvfitsnp, s = "lambda.min")
(cofminnz <- coefmin[coefmin[, 1] != 0,])
```

and

```{r}
coef1se <- coef(cvfitsnp, s = "lambda.1se")
(coef1senz <- coef1se[coef1se[, 1] != 0, ])
```

### Comparison to true values
Since we are working with simulated data, we can compare the above listed results to the true values. The significant SNPs were chosen to be 

```{r}
vec_sign_snp_idx
```

There effect was 

```{r}
vec_snp_eff
```

The number of true significant SNP positions is `r length(vec_sign_snp_idx)`. Because this number is closer to the number of non-zero effects found in the more regularized result, we are comparing the true values to the coefficients in `coef1senz`. 

### SNP-positions with non-zero effect
The variable names that relate to the SNP-positions can be extracted from the result as

```{r}
(s_snp_pos <- gsub(pattern = "V", replacement = "", setdiff(names(coef1senz), "(Intercept)"), fixed = TRUE))
```

The positions found that match the true values are 

```{r}
(vec_match_snp <- intersect(s_snp_pos, as.character(vec_sign_snp_idx)))
```

That means we get `r length(vec_match_snp)` out of a total of `r length(vec_sign_snp_idx)` true significant SNP-positions. This corresponds to a detection rate of `r round(length(vec_match_snp)/length(vec_sign_snp_idx), digits = 2)`. 

### SNP-effects
When comparing the estimated SNP-effects to the true values, the picture is less clear. We are first preparing a dataframe that allows to do the plots.

```{r}
(df_true <- dplyr::data_frame(true_snp_pos = vec_sign_snp_idx[order(vec_sign_snp_idx)],
                                  true_snp_eff = vec_snp_eff[order(vec_sign_snp_idx)]))
```
 
 The same is done for the estimated effects
 
```{r}
(df_est <- dplyr::data_frame(est_snp_pos = as.integer(s_snp_pos), est_snp_eff = as.vector(coef1senz[-1])))
```
 
Now we are joining those which are present in both

```{r}
require(magrittr)
(df_comp_eff <- dplyr::inner_join(df_est, df_true, by = c('est_snp_pos' = 'true_snp_pos')))
```

Showing the comparison in a simple plot

```{r}
ggplot2::qplot(x = true_snp_eff, y = est_snp_eff, data = df_comp_eff, colour = as.factor(est_snp_pos))
```

